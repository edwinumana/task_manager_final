import os
from typing import Dict, Any, Optional, Tuple
import openai
from dotenv import load_dotenv
from app.models.enums import TaskCategory
import tiktoken

# Cargar variables de entorno
load_dotenv()

class AIService:
    """Servicio para interactuar con Azure OpenAI"""
    
    def __init__(self):
        """Inicializa el cliente de Azure OpenAI"""
        try:
            # Verificar si estamos en modo testing
            is_testing = os.getenv("TESTING", "false").lower() == "true" or \
                        os.getenv("FLASK_ENV") == "testing"
            
            if is_testing:
                # En modo testing, usar valores por defecto
                print("И Modo testing detectado - usando configuraci贸n mock para AIService")
                self._setup_testing_mode()
                return
            
            # Obtener las credenciales del archivo .env
            api_key = os.getenv("AZURE_OPENAI_API_KEY")
            api_version = os.getenv("AZURE_OPENAI_API_VERSION")
            azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
            
            # Verificar que todas las credenciales est茅n presentes
            if not all([api_key, api_version, azure_endpoint]):
                raise ValueError("Faltan credenciales de Azure OpenAI en el archivo .env")
            
            # Configurar OpenAI para Azure (versi贸n 0.28.1)
            openai.api_type = "azure"
            openai.api_key = api_key
            openai.api_base = azure_endpoint
            openai.api_version = api_version
            
            self.deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
            if not self.deployment_name:
                raise ValueError("Falta el nombre del deployment en el archivo .env")
            
            self.is_testing = False
            self._setup_production_mode()
            
        except Exception as e:
            print(f"Error al inicializar AIService: {e}")
            raise
    
    def _setup_testing_mode(self):
        """Configura el servicio para modo testing"""
        self.api_key = "test_key"
        self.api_version = "2023-12-01-preview"
        self.azure_endpoint = "https://test.openai.azure.com/"
        self.deployment_name = "test-deployment"
        self.is_testing = True
        
        # Atributos esperados por los tests
        self.client = None  # Mock client
        self.temperature = 0.7
        self.max_tokens = 1000
        
        # Configuraci贸n adicional del modelo (igual que en producci贸n)
        self.top_p = float(os.getenv("TOP_P", "0.2"))
        self.frequency_penalty = float(os.getenv("FREQUENCY_PENALTY", "0.0"))
        self.presence_penalty = float(os.getenv("PRESENCE_PENALTY", "0.0"))
        
        # Configurar encoding para testing
        try:
            self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        except:
            self.encoding = None
    
    def _setup_production_mode(self):
        """Configura el servicio para modo producci贸n"""
        self.is_testing = False
        
        # Atributos esperados por los tests
        self.client = openai  # Cliente real
        self.temperature = 0.7
        self.max_tokens = 1000
        
        # Configuraci贸n adicional del modelo
        self.top_p = float(os.getenv("TOP_P", "0.2"))
        self.frequency_penalty = float(os.getenv("FREQUENCY_PENALTY", "0.0"))
        self.presence_penalty = float(os.getenv("PRESENCE_PENALTY", "0.0"))
        
        # Configurar encoding
        try:
            self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        except:
            self.encoding = None

    def _mock_llm_response(self, system_prompt: str, user_prompt: str) -> Tuple[str, Dict[str, Any]]:
        """Respuesta mock para modo testing"""
        mock_response = f"Mock response for: {user_prompt[:50]}..."
        mock_usage = {
            'prompt_tokens': 50,
            'completion_tokens': 20,
            'total_tokens': 70
        }
        return mock_response, {'usage': mock_usage}

    def _call_llm(self, system_prompt: str, user_prompt: str) -> Tuple[str, Dict[str, Any]]:
        """Llama al LLM de Azure OpenAI"""
        
        # Si estamos en modo testing, usar respuesta mock
        if self.is_testing:
            return self._mock_llm_response(system_prompt, user_prompt)
        
        try:
            # Llamada real a OpenAI
            response = openai.ChatCompletion.create(
                engine=self.deployment_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                top_p=self.top_p,
                frequency_penalty=self.frequency_penalty,
                presence_penalty=self.presence_penalty,
                stop=None
            )
            
            # Extraer la respuesta y estad铆sticas
            content = response.choices[0].message.content.strip()
            usage = response.usage
            
            # Calcular estad铆sticas
            stats = {
                'input_tokens': usage.prompt_tokens,
                'output_tokens': usage.completion_tokens,
                'total_tokens': usage.total_tokens,
                'cost': self.calculate_cost(usage.prompt_tokens, usage.completion_tokens)
            }
            
            return content, stats
            
        except Exception as e:
            # Manejar diferentes tipos de errores de OpenAI
            error_msg = str(e)
            if "rate limit" in error_msg.lower():
                print(f"Error de l铆mite de velocidad: {e}")
                raise Exception("L铆mite de velocidad excedido. Intente nuevamente m谩s tarde.")
            elif "authentication" in error_msg.lower() or "unauthorized" in error_msg.lower():
                print(f"Error de autenticaci贸n: {e}")
                raise Exception("Error de autenticaci贸n con Azure OpenAI.")
            elif "api" in error_msg.lower():
                print(f"Error de API: {e}")
                raise Exception("Error en la API de Azure OpenAI.")
            else:
                print(f"Error inesperado: {e}")
                raise Exception(f"Error inesperado al llamar a la API: {str(e)}")

    def count_tokens(self, text: str) -> int:
        """Cuenta los tokens en un texto"""
        if self.encoding is None:
            # Fallback: aproximaci贸n simple
            return int(len(text.split()) * 1.3)  # Aproximaci贸n
        try:
            return len(self.encoding.encode(text))
        except:
            return int(len(text.split()) * 1.3)  # Fallback

    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Calcula el costo aproximado de la llamada"""
        # Precios aproximados para GPT-3.5-turbo (pueden variar)
        input_cost_per_1k = 0.0015  # $0.0015 por 1K tokens de entrada
        output_cost_per_1k = 0.002  # $0.002 por 1K tokens de salida
        
        input_cost = (input_tokens / 1000) * input_cost_per_1k
        output_cost = (output_tokens / 1000) * output_cost_per_1k
        
        return input_cost + output_cost

    def process_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Procesa una tarea completa con todas las funcionalidades de IA
        
        Args:
            task_data: Diccionario con los datos de la tarea
            
        Returns:
            Dict[str, Any]: Datos de la tarea procesados
        """
        try:
            # Generar descripci贸n
            description, desc_info = self._call_llm(
                "Eres un experto en gesti贸n de tareas. Genera una descripci贸n profesional de m谩ximo 300 palabras a partir de la tarea que te da el usuario.",
                f"Genera una descripci贸n para la tarea: {task_data.get('title', '')}"
            )
            
            # Categorizar
            category, cat_info = self._call_llm(
                "Eres un experto en clasificaci贸n de tareas. Devuelve NICAMENTE una categor铆a a partir de el tipo de tarea y la descrici贸n. La categor铆a debe pertenecer a una de las siguientes opciones: Testing y Control de Calidad, Desarrollo Frontend, Desarrollo Backend, Desarrollo General , Dise帽o de Sistemas, Documentaci贸n, Base de Datos Seguridad, Infraestructura, Mantenimiento, Investigaci贸n, Supervisi贸n, Riesgos Laborales, Limpieza, Otro.",
                f"Categoriza la tarea: {task_data.get('title', '')} - {description}"
            )
            
            # Estimar esfuerzo
            effort, eff_info = self._call_llm(
                "Eres un experto en estimaci贸n de tiempo para la ejecuci贸n de tareas. Calcula el tiempo en horas que toma ejecutar la tarea correspondiente, este dato debe estar entre 2 a 48 horas. Las tareas de desarrollo, control de calidad y testing toman al menos 8 horas, las tarewas de desarrollo de frontend, back end y desarrollo general toman 24 horas, las tarea de documentacion toma 4 horas, la tarea de base de datos toma 16 horas, la tarea de investigaci贸n toma 48 horas, supervisi贸n y riesgos laborales toma 4 horas y otros toma 6 horas Devuelve NICAMENTE un n煤mero de horas.",
                f"Estima las horas para: {task_data.get('title', '')} - {description}"
            )
            
            # Analizar riesgos
            risks, risk_info = self._call_llm(
                "Eres un experto en an谩lisis de riesgos de ejecucion de tareas. Identifica los riesgos potenciales seg煤n la tarea y la descripci贸n de la tarea. Genera una respuesta de m谩ximo 200 palabras.",
                f"Analiza los riesgos de: {task_data.get('title', '')} - {description}"
            )
            
            # Generar mitigaci贸n
            mitigation, mit_info = self._call_llm(
                "Eres un experto en gesti贸n de riesgos de un laboratorio de control de calidad de la industria farmac茅utica. Genera un plan de mitigaci贸n para los riesgos potenciales seg煤n la tarea, su descripci贸n y la descripci贸n de los riesgos. Genera una respuesta de m谩ximo 300 palabras.",
                f"Genera un plan de mitigaci贸n para los siguientes riesgos: {task_data.get('title', '')} - {description} - {risks}"
            )
            
            # Convertir categor铆a a valor interno usando el mapeo del enum
            category_clean = category.strip().lower()
            
            # Primero intentar buscar directamente en los valores del enum
            valid_values = TaskCategory.get_values()
            if category_clean in valid_values:
                category_value = category_clean
            else:
                # Si no se encuentra, intentar mapear desde nombres de visualizaci贸n
                display_names = TaskCategory.get_display_names()
                reverse_mapping = {display_name.lower(): value for value, display_name in display_names.items()}
                category_value = reverse_mapping.get(category_clean, 'otro')
            
            # Actualizar datos de la tarea
            total_tokens = (
                desc_info['total_tokens'] +
                cat_info['total_tokens'] +
                eff_info['total_tokens'] +
                risk_info['total_tokens'] +
                mit_info['total_tokens']
            )
            
            total_cost = (
                desc_info['cost'] +
                cat_info['cost'] +
                eff_info['cost'] +
                risk_info['cost'] +
                mit_info['cost']
            )
            
            task_data.update({
                'description': description,
                'category': category_value,
                'effort': int(effort) if effort.isdigit() else 0,
                'risk_analysis': risks,
                'risk_mitigation': mitigation,
                'tokens_gastados': total_tokens,
                'costos': total_cost,
                'ai_processing': {
                    'description': desc_info,
                    'categorization': cat_info,
                    'effort_estimation': eff_info,
                    'risk_analysis': risk_info,
                    'mitigation': mit_info
                }
            })
            
            return task_data
            
        except Exception as e:
            raise Exception(f"Error al procesar la tarea: {str(e)}")

    def generate_description(self, title: str) -> Dict[str, Any]:
        """
        Genera una descripci贸n detallada para una tarea
        
        Args:
            title: T铆tulo de la tarea
            
        Returns:
            Dict[str, Any]: Descripci贸n generada y metadatos
        """
        try:
            description, token_info = self._call_llm(
                "Eres un experto en gesti贸n de tareas de control de calidad. Genera una descripci贸n profesional de m谩ximo 200 palabras.",
                f"Genera una descripci贸n para la tarea: {title}"
            )
            
            # Agregar logs para depuraci贸n
            print("\n=== Datos generados en generate_description ===")
            print(f"Token info: {token_info}")
            print(f"Tokens gastados: {token_info['total_tokens']}")
            print(f"Costos: {token_info['cost']}")
            
            response = {
                'success': True,
                'description': description,
                'total_tokens': token_info['total_tokens'],
                'cost': token_info['cost']
            }
            
            print(f"Response a devolver: {response}")
            print("===========================================\n")
            
            return response
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def categorize_task(self, title: str, description: str = '') -> Dict[str, Any]:
        """
        Categoriza una tarea usando IA
        
        Args:
            title: T铆tulo de la tarea
            description: Descripci贸n de la tarea (opcional)
            
        Returns:
            Dict[str, Any]: Categor铆a generada y metadatos
        """
        try:
            category, token_info = self._call_llm(
                "Eres un experto en clasificaci贸n de tareas. Devuelve NICAMENTE una categor铆a a partir de el tipo de tarea y la descripci贸n. La categor铆a debe pertenecer a una de las siguientes opciones: Testing y Control de Calidad, Desarrollo Frontend, Desarrollo Backend, Desarrollo General , Dise帽o de Sistemas, Documentaci贸n, Base de Datos Seguridad, Infraestructura, Mantenimiento, Investigaci贸n, Supervisi贸n, Riesgos Laborales, Limpieza, Otro.",
                f"Categoriza la tarea: {title} - {description}"
            )
            
            # Limpiar la categor铆a
            category = category.strip()
            
            # Usar el mapeo del enum para convertir nombres de visualizaci贸n a valores internos
            display_names = TaskCategory.get_display_names()
            # Crear mapeo inverso: nombre de visualizaci贸n -> valor interno
            reverse_mapping = {display_name: value for value, display_name in display_names.items()}
            
            # Convertir la categor铆a al valor interno
            select_value = reverse_mapping.get(category, 'otro')
            
            return {
                'success': True,
                'category': select_value,
                'total_tokens': token_info['total_tokens'],
                'cost': token_info['cost']
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def estimate_effort(self, title: str, description: str = '', category: str = '') -> Dict[str, Any]:
        """
        Estima el esfuerzo necesario para una tarea
        
        Args:
            title: T铆tulo de la tarea
            description: Descripci贸n de la tarea (opcional)
            category: Categor铆a de la tarea (opcional)
            
        Returns:
            Dict[str, Any]: Estimaci贸n de esfuerzo y metadatos
        """
        try:
            effort, token_info = self._call_llm(
                "Eres un experto en estimaci贸n de tiempo para la ejecuci贸n de tareas. Calcula el tiempo en horas que toma ejecutar la tarea correspondiente, este dato debe estar entre 2 a 48 horas. Las tareas de desarrollo, control de calidad y testing toman al menos 8 horas, las tarewas de desarrollo de frontend, back end y desarrollo general toman 24 horas, la tarea de documentacion toma 4 horas, la tarea de base de datos toma 16 horas, la tarea de investigaci贸n toma 48 horas, supervisi贸n y riesgos laborales toma 4 horas y otros toma 6 horas Devuelve NICAMENTE un n煤mero de horas.",
                f"Estima las horas para: {title} - {description} - {category}"
            )
            
            # Limpiar y convertir el esfuerzo a entero
            effort = effort.strip()
            try:
                effort = int(effort)
            except ValueError:
                effort = 0
            
            return {
                'success': True,
                'effort': effort,
                'total_tokens': token_info['total_tokens'],
                'cost': token_info['cost']
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def analyze_risks(self, title: str, description: str = '', category: str = '') -> Dict[str, Any]:
        """
        Analiza los riesgos de una tarea
        
        Args:
            title: T铆tulo de la tarea
            description: Descripci贸n de la tarea (opcional)
            category: Categor铆a de la tarea (opcional)
            
        Returns:
            Dict[str, Any]: An谩lisis de riesgos y metadatos
        """
        try:
            risks, token_info = self._call_llm(
                "Eres un experto en an谩lisis de riesgos que se presentan en la ejecuci贸n de tareas. Identifica los riesgos potenciales seg煤n la tarea y la descripci贸n de la tarea. Genera una respuesta de m谩ximo 200 palabras.",
                f"Analiza los riesgos de: {title} - {description} - {category}"
            )
            
            return {
                'success': True,
                'risk_analysis': risks,
                'total_tokens': token_info['total_tokens'],
                'cost': token_info['cost']
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def generate_mitigation(self, title: str, description: str = '', category: str = '', risk_analysis: str = '') -> Dict[str, Any]:
        """
        Genera un plan de mitigaci贸n para los riesgos de una tarea
        
        Args:
            title: T铆tulo de la tarea
            description: Descripci贸n de la tarea (opcional)
            category: Categor铆a de la tarea (opcional)
            risk_analysis: An谩lisis de riesgos (opcional)
            
        Returns:
            Dict[str, Any]: Plan de mitigaci贸n y metadatos
        """
        try:
            mitigation, token_info = self._call_llm(
                "Eres un experto en gesti贸n de riesgos en la ejecuci貌n de tareas. Genera un plan de mitigaci贸n para los riesgos potenciales seg煤n la tarea, su descripci贸n y la descripci贸n de los riesgos. Genera una respuesta de m谩ximo 200 palabras.",
                f"Genera un plan de mitigaci贸n para los siguientes riesgos: {title} - {description} - {category} - {risk_analysis}"
            )
            
            return {
                'success': True,
                'mitigation_plan': mitigation,
                'total_tokens': token_info['total_tokens'],
                'cost': token_info['cost']
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            } 

    def generate_user_story(self, prompt: str) -> dict:
        """Genera una historia de usuario en espa帽ol usando Azure OpenAI y devuelve un dict."""
        system_prompt = (
            "Eres un experto en gesti贸n 谩gil de proyectos. Genera una historia de usuario en formato JSON con los campos: project, role, goal, reason, description, priority (baja, media, alta, bloqueante), story_points (1-8), effort_hours (decimal). Responde solo el JSON, sin explicaciones."
        )
        response_text, _ = self._call_llm(system_prompt, prompt)
        import json
        try:
            return json.loads(response_text)
        except Exception:
            # Si la respuesta no es JSON v谩lido, intentar extraer el bloque JSON
            import re
            match = re.search(r'\{[\s\S]*\}', response_text)
            if match:
                return json.loads(match.group(0))
            raise ValueError(f"Respuesta de IA no es JSON v谩lido: {response_text}")

    def generate_tasks(self, prompt: str) -> list:
        """Genera una lista de tareas en espa帽ol usando Azure OpenAI y devuelve una lista de dicts."""
        system_prompt = (
            "Eres un experto en gesti贸n de un laboratorio de control de calidad para la industria farmac茅utica. "
            "Genera exactamente 5 tareas en formato JSON para una historia de usuario. "
            "Cada tarea debe tener un t铆tulo concreto de m谩ximo 30 palabras y una descripci贸n de m谩ximo 100 palabras. "
            "El formato debe ser: [{\"title\": \"T铆tulo de la tarea\", \"description\": \"Descripci贸n de la tarea\"}, ...]. "
            "Responde solo el JSON, sin explicaciones."
        )
        response_text, _ = self._call_llm(system_prompt, prompt)
        import json
        try:
            tasks = json.loads(response_text)
            # Asegurar que devolvemos una lista
            if isinstance(tasks, list):
                return tasks
            else:
                # Si no es una lista, intentar convertir
                return [tasks] if isinstance(tasks, dict) else []
        except Exception as e:
            print(f"Error parsing JSON response: {e}")
            print(f"Response text: {response_text}")
            # Si la respuesta no es JSON v谩lido, intentar extraer el bloque JSON
            import re
            match = re.search(r'\[[\s\S]*\]', response_text)
            if match:
                try:
                    return json.loads(match.group(0))
                except:
                    pass
            # Fallback: crear tareas b谩sicas
            return [
                {"title": "Tarea 1", "description": "Descripci贸n de la tarea 1"},
                {"title": "Tarea 2", "description": "Descripci贸n de la tarea 2"},
                {"title": "Tarea 3", "description": "Descripci贸n de la tarea 3"},
                {"title": "Tarea 4", "description": "Descripci贸n de la tarea 4"},
                {"title": "Tarea 5", "description": "Descripci贸n de la tarea 5"}
            ] 